{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) Preprocessing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure dataset file are in correct location.\n",
    "run preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.LDA import preprocess_text\n",
    "from train.saving import save_model_results\n",
    "from model.mf import MF_Bias, LDANet\n",
    "from model.utility import RMSELoss\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from train.saving import save_model_results\n",
    "from skorch import NeuralNetRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jb/Documents/GitHub/Intro ML (Nick Pang) Repository/rl-recommender\n"
     ]
    }
   ],
   "source": [
    "subsets = {\n",
    "        \"Toys_and_Games_5.json\",\n",
    "        \"Apps_for_Android_5.json\",\n",
    "        \"Health_and_Personal_Care_5.json\",\n",
    "    }\n",
    "pth = os.getcwd()[:-4]\n",
    "NUM_TOPICS = 10\n",
    "#preprocess_text(dataset=subsets, pth=pth, n_topics=NUM_TOPICS)\n",
    "print(pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TG5_df = pd.read_json(os.path.join(pth, \"datasets\", \"raw\",\"Toys_and_Games_5.json\"), lines=True)\n",
    "AA5_df = pd.read_json(os.path.join(pth, \"datasets\", \"raw\", \"Apps_for_Android_5.json\"), lines=True)\n",
    "HPC_df = pd.read_json(os.path.join(pth, \"datasets\", \"raw\", \"Health_and_Personal_Care_5.json\"), lines=True)\n",
    "\n",
    "df = pd.concat([TG5_df, AA5_df, HPC_df], axis=0)\n",
    "del TG5_df, AA5_df, HPC_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        reviewerID   asin  overall\n",
      "0           124394  31743      5.0\n",
      "1           124395  31743      4.0\n",
      "2           124396  31743      5.0\n",
      "3           124397  31743      5.0\n",
      "4           124398  31743      4.0\n",
      "...            ...    ...      ...\n",
      "346350       29878  18533      5.0\n",
      "346351       37951  18533      5.0\n",
      "346352       36501  18533      5.0\n",
      "346353       36040  18533      5.0\n",
      "346354       36504  18533      5.0\n",
      "\n",
      "[1266889 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Keep essentials only\n",
    "df = df[['reviewerID', 'asin', 'overall']]\n",
    "\n",
    "# Map values\n",
    "user_dict = pd.read_csv(os.path.join(pth, \"datasets\", \"processed\", \"lda\", \"user_mappings.csv\"))\n",
    "item_dict = pd.read_csv(os.path.join(pth, \"datasets\", \"processed\", \"lda\", \"item_mappings.csv\"))\n",
    "user_dict = dict(zip(user_dict.iloc[:, 1], user_dict.index))\n",
    "item_dict = dict(zip(item_dict.iloc[:, 1], item_dict.index))\n",
    "\n",
    "df['reviewerID'] = df['reviewerID'].map(user_dict).fillna(df['reviewerID'])\n",
    "df['asin'] = df['asin'].map(item_dict).fillna(df['asin'])\n",
    "\n",
    "df[['reviewerID', 'asin']] = df[['reviewerID', 'asin']].astype(int)\n",
    "df['overall'] = df['overall'].astype(float)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = df['overall'].mean()\n",
    "np.savez_compressed(os.path.join(pth, \"datasets\", \"processed\", \"Subset_5core_PreprocessLDA.npz\"),\n",
    "                    x = df[['reviewerID', 'asin']],\n",
    "                    y = df['overall'],\n",
    "                    u_size = len(user_dict),\n",
    "                    i_size = len(item_dict),\n",
    "                    m = m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) Create Model\n",
    "\n",
    "This model doesn't use internal LDA YET!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I wanted to test running on Apple Silicon Chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_type: mps\n",
      "float_type: torch.float32\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('mps' if torch.has_mps else 'cpu')\n",
    "f_type = torch.float32 if device.type == 'mps' else torch.float64\n",
    "print(f\"device_type: {device}\")\n",
    "print(f\"float_type: {f_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.0005\n",
    "EPOCHS = 15\n",
    "DECAY = 1e-3\n",
    "DROPOUT = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "data = np.load(os.path.join(pth, \"datasets\", \"processed\", \"Subset_5core_PreprocessLDA.npz\"))\n",
    "x = data['x']\n",
    "y = data['y']\n",
    "U_size = data['u_size']\n",
    "I_size = data['i_size']\n",
    "G_b = data['m']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)\n",
    "y_train, y_test = torch.tensor(y_train, dtype = f_type).to(device), torch.tensor(y_test, dtype = f_type).to(device)\n",
    "\n",
    "loss_fn = RMSELoss()\n",
    "optimizer = Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I am adding the class here only so I append **.to(*device*)** for each tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF(nn.Module):\n",
    "    def __init__(self, n_users, n_items, K, dropout=0):\n",
    "        super().__init__()\n",
    "        self.user_m = nn.Embedding(\n",
    "            n_users, K, dtype=f_type\n",
    "        ).to(device)  # can include option sparse = True for memory\n",
    "        self.item_m = nn.Embedding(n_items, K, dtype=f_type).to(device)\n",
    "        self.drop_u = nn.Dropout(dropout)\n",
    "        self.drop_i = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        user_ids = x[:, 0]\n",
    "        item_ids = x[:, 1]\n",
    "        user_embeds = self.drop_u(self.user_m(user_ids))\n",
    "        item_embeds = self.drop_i(self.item_m(item_ids))\n",
    "        prod = user_embeds * item_embeds\n",
    "\n",
    "        out = torch.sum(prod, 1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# Matrix factorization with user/item biases\n",
    "class MF_Bias(MF):\n",
    "    def __init__(self, n_users, n_items, K, G_b, dropout=0):\n",
    "        super().__init__(n_users, n_items, K, dropout)\n",
    "\n",
    "        self.user_b = nn.Embedding(n_users, 1, dtype=f_type).to(device)\n",
    "        self.item_b = nn.Embedding(n_items, 1, dtype=f_type).to(device)\n",
    "        nn.init.zeros_(self.user_b.weight)\n",
    "        nn.init.zeros_(self.item_b.weight)\n",
    "\n",
    "        self.G_b = torch.from_numpy(G_b)\n",
    "\n",
    "    def forward(self, x):\n",
    "        user_ids = x[:, 0]\n",
    "        item_ids = x[:, 1]\n",
    "        out = super().forward(x)\n",
    "\n",
    "        user_biases = self.user_b(user_ids).squeeze()\n",
    "        item_biases = self.item_b(item_ids).squeeze()\n",
    "\n",
    "        out += user_biases + item_biases + self.G_b\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MF_Bias(U_size, I_size, NUM_TOPICS, G_b, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_map = pd.read_csv(os.path.join(pth, \"datasets\", \"processed\", \"lda\", \"user_topics.csv\"))\n",
    "for idx,row  in user_map.iterrows():\n",
    "    row = row[1:] # to remove 'User_ID' from row\n",
    "    model.user_m.weight.data[idx] = torch.tensor(row.values, dtype=f_type).to(device)\n",
    "\n",
    "item_map = pd.read_csv(os.path.join(pth, \"datasets\", \"processed\", \"lda\", \"item_topics.csv\"))\n",
    "for idx,row  in item_map.iterrows():\n",
    "    row = row[1:] # to remove 'Item_ID' from row\n",
    "    model.item_m.weight.data[idx] = torch.tensor(row.values, dtype=f_type).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor = NeuralNetRegressor(\n",
    "    model,\n",
    "    criterion = loss_fn,\n",
    "    optimizer = optimizer,\n",
    "    optimizer__param_groups = [\n",
    "        ('user_m.weight', {'weight_decay': DECAY}),\n",
    "        ('item_m.weight', {'weight_decay': DECAY})\n",
    "    ],\n",
    "    optimizer__lr = LEARNING_RATE,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    max_epochs = EPOCHS\n",
    ")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Borrowed from *save_model_results()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss       dur\n",
      "-------  ------------  ------------  --------\n",
      "      1        \u001b[36m1.2336\u001b[0m        \u001b[32m1.1960\u001b[0m  100.0761\n",
      "      2        \u001b[36m1.1683\u001b[0m        \u001b[32m1.1693\u001b[0m  106.0843\n",
      "      3        \u001b[36m1.1339\u001b[0m        \u001b[32m1.1529\u001b[0m  100.7080\n",
      "      4        \u001b[36m1.1083\u001b[0m        \u001b[32m1.1421\u001b[0m  100.5575\n",
      "      5        \u001b[36m1.0884\u001b[0m        \u001b[32m1.1347\u001b[0m  95.8565\n",
      "      6        \u001b[36m1.0722\u001b[0m        \u001b[32m1.1295\u001b[0m  110.8181\n",
      "      7        \u001b[36m1.0588\u001b[0m        \u001b[32m1.1259\u001b[0m  97.7337\n",
      "      8        \u001b[36m1.0475\u001b[0m        \u001b[32m1.1233\u001b[0m  98.0689\n",
      "      9        \u001b[36m1.0379\u001b[0m        \u001b[32m1.1215\u001b[0m  92.8086\n",
      "     10        \u001b[36m1.0295\u001b[0m        \u001b[32m1.1203\u001b[0m  90.6047\n",
      "     11        \u001b[36m1.0221\u001b[0m        \u001b[32m1.1195\u001b[0m  89.3275\n",
      "     12        \u001b[36m1.0156\u001b[0m        \u001b[32m1.1190\u001b[0m  88.0651\n",
      "     13        \u001b[36m1.0099\u001b[0m        \u001b[32m1.1189\u001b[0m  91.5750\n",
      "     14        \u001b[36m1.0047\u001b[0m        1.1190  92.5040\n",
      "     15        \u001b[36m1.0001\u001b[0m        1.1192  91.8423\n"
     ]
    }
   ],
   "source": [
    "results = regressor.fit(x_train, y_train)\n",
    "history = results.history\n",
    "\n",
    "tr_losses = [i['train_loss'] for i in history]\n",
    "t_losses = [i['valid_loss'] for i in history]\n",
    "\n",
    "with open(os.path.join(pth, \"results\", f\"model_PreprocessingLDA_{device}_{f_type}.pkl\"), 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "np.savez_compressed(os.path.join(pth, \"results\", f\"loss_PreprocessingLDA_{device}_{f_type}.npz\"),\n",
    "                    tr_loss = tr_losses,\n",
    "                    t_loss = t_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (4) Graphs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL-Recommender-Git",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
